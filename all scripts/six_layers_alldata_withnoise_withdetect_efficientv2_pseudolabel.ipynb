{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing modules\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# SETUP\n",
    "################################################################################\n",
    "\n",
    "# Convenience and saving flags\n",
    "ABRIDGED_RUN = True\n",
    "SAVE_AFTER_TRAINING = True # Save the model when you are done\n",
    "SAVE_CHECKPOINTS = True # Save the model after ever epoch\n",
    "REPORT_TRAINING_LOSS_PER_EPOCH = True # Track the training loss each epoch, and write it to a file after training\n",
    "REPORT_VALIDATION_LOSS_PER_EPOCH = True # Lets us make a nice learning curve after training\n",
    "\n",
    "# Training hyperparameters\n",
    "BATCH_SIZE = 96 # Number of samples per batch while training our network\n",
    "NUM_EPOCHS = 10 # Number of epochs to train our network\n",
    "LEARNING_RATE = 0.001 # Learning rate for our optimizer\n",
    "\n",
    "# Directories\n",
    "CHECKPOINT_DIR = \"checkpoints/\" # Checkpoints, models, and training data will be saved here\n",
    "DATA_DIR = \"../data/\"\n",
    "AUDIO_DIR = DATA_DIR + \"train_audio/\"\n",
    "UNLABELED_DIR = DATA_DIR + \"unlabeled_soundscapes/\"\n",
    "\n",
    "# CURRENT PIPELINE:\n",
    "#  - Split each audio file into 5 second clips\n",
    "#  - Discard any scrap with duration less than 3.5s. Pad others.\n",
    "#  - Run Birdcall detection on each clip and change labels appropriately\n",
    "#  - Loss function is CrossEntropyLoss\n",
    "#  - Train with freq/time masking, random power, and pink bg noise.\n",
    "#  - Validate without freq/time masking, random power, or bg noise.\n",
    "#  - Model output will be a vector of logits. Need to apply sigmoid to get probabilities.\n",
    "\n",
    "\n",
    "MODEL_NAME = \"CE_ALLDATA_PINKBG_WITHDETECT_RANDOMSAMPLE_PSEUDOLABEL\"\n",
    "DETECTION_MODEL = 'BIRDCALL_DETECTION_DCASE_FINAL'\n",
    "\n",
    "# Preprocessing info\n",
    "SAMPLE_RATE = 32000 # All our audio uses this sample rate\n",
    "SAMPLE_LENGTH = 5 # Duration we want to crop our audio to\n",
    "NUM_SPECIES = 182 # Number of bird species we need to label\n",
    "MIN_SAMPLE_LENGTH = 3.5 # Only use samples with length >= 3.5 seconds\n",
    "\n",
    "# Min and max signal to noise ratio\n",
    "MAX_SNR = 20\n",
    "MIN_SNR = 10\n",
    "\n",
    "################################################################################\n",
    "# IMPORTS\n",
    "################################################################################\n",
    "\n",
    "print(\"Importing modules\")\n",
    "\n",
    "# Basic imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import librosa\n",
    "import random\n",
    "import os\n",
    "import IPython.display as ipd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Torch imports\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchaudio.transforms import MelSpectrogram, Resample\n",
    "from IPython.display import Audio\n",
    "import torch.optim as optim\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# LOAD DATA\n",
    "################################################################################\n",
    "\n",
    "data = pd.read_csv(DATA_DIR+\"full_metadata.csv\")\n",
    "data['filepath'] = AUDIO_DIR + data['filename']\n",
    "\n",
    "# We only need the filepath, primary label, and duration(?)\n",
    "data = data[['filepath', 'primary_label', 'duration']]\n",
    "\n",
    "# Replace string labels by tensors whose entries are dummies\n",
    "species = ['asbfly', 'ashdro1', 'ashpri1', 'ashwoo2', 'asikoe2', 'asiope1', 'aspfly1', 'aspswi1', 'barfly1', 'barswa', 'bcnher', 'bkcbul1', 'bkrfla1', 'bkskit1', 'bkwsti', 'bladro1', 'blaeag1', 'blakit1', 'blhori1', 'blnmon1', 'blrwar1', 'bncwoo3', 'brakit1', 'brasta1', 'brcful1', 'brfowl1', 'brnhao1', 'brnshr', 'brodro1', 'brwjac1', 'brwowl1', 'btbeat1', 'bwfshr1', 'categr', 'chbeat1', 'cohcuc1', 'comfla1', 'comgre', 'comior1', 'comkin1', 'commoo3', 'commyn', 'compea', 'comros', 'comsan', 'comtai1', 'copbar1', 'crbsun2', 'cregos1', 'crfbar1', 'crseag1', 'dafbab1', 'darter2', 'eaywag1', 'emedov2', 'eucdov', 'eurbla2', 'eurcoo', 'forwag1', 'gargan', 'gloibi', 'goflea1', 'graher1', 'grbeat1', 'grecou1', 'greegr', 'grefla1', 'grehor1', 'grejun2', 'grenig1', 'grewar3', 'grnsan', 'grnwar1', 'grtdro1', 'gryfra', 'grynig2', 'grywag', 'gybpri1', 'gyhcaf1', 'heswoo1', 'hoopoe', 'houcro1', 'houspa', 'inbrob1', 'indpit1', 'indrob1', 'indrol2', 'indtit1', 'ingori1', 'inpher1', 'insbab1', 'insowl1', 'integr', 'isbduc1', 'jerbus2', 'junbab2', 'junmyn1', 'junowl1', 'kenplo1', 'kerlau2', 'labcro1', 'laudov1', 'lblwar1', 'lesyel1', 'lewduc1', 'lirplo', 'litegr', 'litgre1', 'litspi1', 'litswi1', 'lobsun2', 'maghor2', 'malpar1', 'maltro1', 'malwoo1', 'marsan', 'mawthr1', 'moipig1', 'nilfly2', 'niwpig1', 'nutman', 'orihob2', 'oripip1', 'pabflo1', 'paisto1', 'piebus1', 'piekin1', 'placuc3', 'plaflo1', 'plapri1', 'plhpar1', 'pomgrp2', 'purher1', 'pursun3', 'pursun4', 'purswa3', 'putbab1', 'redspu1', 'rerswa1', 'revbul', 'rewbul', 'rewlap1', 'rocpig', 'rorpar', 'rossta2', 'rufbab3', 'ruftre2', 'rufwoo2', 'rutfly6', 'sbeowl1', 'scamin3', 'shikra1', 'smamin1', 'sohmyn1', 'spepic1', 'spodov', 'spoowl1', 'sqtbul1', 'stbkin1', 'sttwoo1', 'thbwar1', 'tibfly3', 'tilwar1', 'vefnut1', 'vehpar1', 'wbbfly1', 'wemhar1', 'whbbul2', 'whbsho3', 'whbtre1', 'whbwag1', 'whbwat1', 'whbwoo2', 'whcbar1', 'whiter2', 'whrmun', 'whtkin2', 'woosan', 'wynlau1', 'yebbab1', 'yebbul3', 'zitcis1']\n",
    "\n",
    "species_to_index = {species[i]:i for i in range(len(species))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# GENERATING PSEUDOLABELS ON UNLABELED DATA VIA TRANSFER LEARNING \n",
    "################################################################################\n",
    "\n",
    "# Using ideas from \"Transfer Learning with Pseudo Multi-Label Birdcall Classification for DS@GT BirdCLEF 2024\" \n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "google_model = hub.load('https://www.kaggle.com/models/google/bird-vocalization-classifier/TensorFlow2/bird-vocalization-classifier/8')\n",
    "google_labels_path = hub.resolve('https://kaggle.com/models/google/bird-vocalization-classifier/frameworks/tensorFlow2/variations/bird-vocalization-classifier/versions/8') + \"/assets/label.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_labels = pd.read_csv(google_labels_path)\n",
    "labels_2021 = set(google_labels['ebird2021'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:21<00:00,  2.19it/s]\n",
      "100%|██████████| 48/48 [00:21<00:00,  2.20it/s]\n",
      "100%|██████████| 48/48 [00:21<00:00,  2.19it/s]\n",
      "100%|██████████| 48/48 [00:21<00:00,  2.19it/s]\n",
      "100%|██████████| 48/48 [00:21<00:00,  2.24it/s]\n",
      "100%|██████████| 48/48 [00:21<00:00,  2.22it/s]\n",
      "100%|██████████| 48/48 [00:21<00:00,  2.22it/s]\n",
      "100%|██████████| 48/48 [00:21<00:00,  2.22it/s]\n",
      "100%|██████████| 48/48 [00:21<00:00,  2.20it/s]\n",
      "100%|██████████| 10/10 [03:16<00:00, 19.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 0 datapoints from unlabeled soundscape\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "unlabeled_soundscapes = sorted(os.listdir(UNLABELED_DIR))\n",
    "step_size = SAMPLE_RATE * 5\n",
    "count = 0 \n",
    "\n",
    "if ABRIDGED_RUN == True:\n",
    "    unlabeled_soundscapes = random.sample(unlabeled_soundscapes, 10)\n",
    "else:\n",
    "    unlabeled_soundscapes = random.sample(unlabeled_soundscapes, 800)\n",
    "    \n",
    "for path in tqdm(unlabeled_soundscapes):\n",
    "    num_segments = 240 // 5\n",
    "    waveform, sr = librosa.load(UNLABELED_DIR + path, sr=SAMPLE_RATE)\n",
    "    waveform = waveform.astype(np.float32)\n",
    "    dur = librosa.get_duration(y=waveform, sr=sr)\n",
    "    \n",
    "    \n",
    "    if len(waveform) < 240 * SAMPLE_RATE:\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    for i in tqdm(range(num_segments)):\n",
    "        x = waveform[(step_size*i):(step_size*(i+1))]\n",
    "        logits = google_model.infer_tf(x[np.newaxis, :])\n",
    "        logits = logits['order'].numpy()\n",
    "        pred_label = google_labels.iloc[[np.argmax(logits)]]['ebird2021'].item() \n",
    "        \n",
    "        if pred_label in species: \n",
    "            print(\"Adding new row to metadata\")\n",
    "            count += 1\n",
    "            new_row = {\n",
    "            'filepath': UNLABELED_DIR + path,\n",
    "            'primary_label': pred_label,\n",
    "            'duration': dur\n",
    "            }\n",
    "            data = pd.concat([data, pd.DataFrame([new_row])])\n",
    "            break \n",
    "    \n",
    "\n",
    "print(f\"Added {count} datapoints from unlabeled soundscape\")\n",
    "\n",
    "# for path in tqdm(unlabeled_soundscapes):\n",
    "#     waveform, sr = librosa.load(UNLABELED_DIR + path, sr=SAMPLE_RATE)\n",
    "#     waveform = waveform.astype(np.float32)\n",
    "#     dur = librosa.get_duration(y=waveform, sr=sr)\n",
    "    \n",
    "    \n",
    "#     if len(waveform) < 240 * SAMPLE_RATE:\n",
    "#         continue\n",
    "    \n",
    "    \n",
    "#     for i in range(4):\n",
    "#         start = np.random.randint(waveform.shape[0]-SAMPLE_RATE*SAMPLE_LENGTH+1)\n",
    "\n",
    "#         x = waveform[start:start + SAMPLE_RATE*SAMPLE_LENGTH]\n",
    "#         logits = google_model.infer_tf(x[np.newaxis, :])\n",
    "#         logits = logits['order'].numpy()\n",
    "#         pred_label = google_labels.iloc[[np.argmax(logits)]]['ebird2021'].item() \n",
    "        \n",
    "#         if pred_label in species: \n",
    "#             print(\"Adding new row to metadata\")\n",
    "#             count += 1\n",
    "#             new_row = {\n",
    "#             'filepath': UNLABELED_DIR + path,\n",
    "#             'primary_label': pred_label,\n",
    "#             'duration': dur\n",
    "#             }\n",
    "\n",
    "#         data = pd.concat([data, pd.DataFrame([new_row])])\n",
    "    \n",
    "\n",
    "\n",
    "# print(f\"Added {count} datapoints from unlabeled soundscape\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['index_label'] = data['primary_label'].apply(lambda x: species_to_index[x])\n",
    "data['tensor_label'] = pd.Series(pd.get_dummies(data['primary_label']).astype(int).values.tolist()).apply(lambda x: torch.Tensor(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'afcdov1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex_label\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprimary_label\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mspecies_to_index\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensor_label\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(pd\u001b[38;5;241m.\u001b[39mget_dummies(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprimary_label\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist())\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: torch\u001b[38;5;241m.\u001b[39mTensor(x))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Remove overly short clips\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[105], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex_label\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprimary_label\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mspecies_to_index\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      2\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensor_label\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(pd\u001b[38;5;241m.\u001b[39mget_dummies(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprimary_label\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist())\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: torch\u001b[38;5;241m.\u001b[39mTensor(x))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Remove overly short clips\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'afcdov1'"
     ]
    }
   ],
   "source": [
    "# Remove overly short clips\n",
    "data = data[data['duration'] >= MIN_SAMPLE_LENGTH]\n",
    "\n",
    "# Use 100 rows of data for quick runs to test functionalities\n",
    "if ABRIDGED_RUN == True:\n",
    "    data = data.sample(100)\n",
    "\n",
    "# Progress bars for loading data to memory\n",
    "tqdm.pandas()\n",
    "\n",
    "# print(\"Loading nocall background noise snippets into memory\")\n",
    "# nocalls = pd.read_csv(f\"{DATA_DIR}nocall_snippets/filenames.csv\")\n",
    "# def load_nocall(filepath):\n",
    "#    signal, _ = torchaudio.load(filepath)\n",
    "    # Reduce to one channel\n",
    "#    signal = torch.mean(signal,dim = 0).unsqueeze(0)\n",
    "#    return signal\n",
    "# nocalls['signal'] = nocalls['filename'].progress_apply(lambda x: load_nocall(f\"{DATA_DIR}nocall_snippets/{x}\"))\n",
    "# print(\"Done\")\n",
    "\n",
    "print(\"Loading training audio signals into memory\")\n",
    "# Loads signal to memory, pads to 5 seconds, cuts to 60 seconds\n",
    "def filepath_to_signal(filepath):\n",
    "    signal, _ = torchaudio.load(filepath)\n",
    "    return signal\n",
    "data['signal'] = data['filepath'].progress_apply(filepath_to_signal)\n",
    "print(\"Done\")\n",
    "\n",
    "# Train test split, stratified by species\n",
    "stratify = data['primary_label']\n",
    "if ABRIDGED_RUN == True:\n",
    "    stratify = None\n",
    "data_train, data_validation = train_test_split(data, test_size = 0.2, stratify=stratify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "      <th>primary_label</th>\n",
       "      <th>duration</th>\n",
       "      <th>index_label</th>\n",
       "      <th>tensor_label</th>\n",
       "      <th>signal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>../data/train_audio/barswa/XC176091.ogg</td>\n",
       "      <td>barswa</td>\n",
       "      <td>7.209000</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[tensor(4.2448e-06), tensor(2.3316e-07), tens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3275</th>\n",
       "      <td>../data/train_audio/blrwar1/XC199244.ogg</td>\n",
       "      <td>blrwar1</td>\n",
       "      <td>81.240813</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[tensor(-2.4091e-05), tensor(-3.9600e-06), te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9185</th>\n",
       "      <td>../data/train_audio/eucdov/XC410909.ogg</td>\n",
       "      <td>eucdov</td>\n",
       "      <td>56.928000</td>\n",
       "      <td>55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[tensor(-3.5863e-05), tensor(-7.7408e-06), te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11070</th>\n",
       "      <td>../data/train_audio/grecou1/XC451921.ogg</td>\n",
       "      <td>grecou1</td>\n",
       "      <td>17.040000</td>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[tensor(3.1221e-05), tensor(1.3593e-07), tens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13559</th>\n",
       "      <td>../data/train_audio/grywag/XC793706.ogg</td>\n",
       "      <td>grywag</td>\n",
       "      <td>12.068563</td>\n",
       "      <td>76</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[tensor(2.4629e-05), tensor(1.3022e-05), tens...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       filepath primary_label   duration  \\\n",
       "722     ../data/train_audio/barswa/XC176091.ogg        barswa   7.209000   \n",
       "3275   ../data/train_audio/blrwar1/XC199244.ogg       blrwar1  81.240813   \n",
       "9185    ../data/train_audio/eucdov/XC410909.ogg        eucdov  56.928000   \n",
       "11070  ../data/train_audio/grecou1/XC451921.ogg       grecou1  17.040000   \n",
       "13559   ../data/train_audio/grywag/XC793706.ogg        grywag  12.068563   \n",
       "\n",
       "       index_label tensor_label  \\\n",
       "722              9          NaN   \n",
       "3275            20          NaN   \n",
       "9185            55          NaN   \n",
       "11070           64          NaN   \n",
       "13559           76          NaN   \n",
       "\n",
       "                                                  signal  \n",
       "722    [[tensor(4.2448e-06), tensor(2.3316e-07), tens...  \n",
       "3275   [[tensor(-2.4091e-05), tensor(-3.9600e-06), te...  \n",
       "9185   [[tensor(-3.5863e-05), tensor(-7.7408e-06), te...  \n",
       "11070  [[tensor(3.1221e-05), tensor(1.3593e-07), tens...  \n",
       "13559  [[tensor(2.4629e-05), tensor(1.3022e-05), tens...  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# PREPROCESSING FUNCTIONS\n",
    "################################################################################\n",
    "\n",
    "# Transforms audio signal to a spectrogram\n",
    "spectrogram_transform = torchaudio.transforms.Spectrogram(\n",
    "        n_fft=2048,\n",
    "        win_length=2048,\n",
    "        hop_length=512,\n",
    "        power=2\n",
    "    )\n",
    "\n",
    "# Converts ordinary spectrogram to Mel scale\n",
    "mel_spectrogram_transform = torchaudio.transforms.MelScale(\n",
    "    n_mels=128,\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    f_min=40,\n",
    "    f_max=16000,\n",
    "    n_stft=1025  # the number of frequency bins in the spectrogram\n",
    ")\n",
    "\n",
    "# Scales decibels to reasonable level (apply to a spectrogram or Mel spectrogram)\n",
    "db_scaler = torchaudio.transforms.AmplitudeToDB(stype=\"power\", top_db=80)\n",
    "\n",
    "# Resizes spectrograms into square images\n",
    "resize = transforms.Resize((224, 224), antialias = None)\n",
    "\n",
    "# Applies a frequency mask to a spectrogram\n",
    "def freq_mask(spec, F=30):\n",
    "    num_mel_channels = spec.shape[1]\n",
    "    f = random.randrange(0, F)\n",
    "    f_zero = random.randrange(0, num_mel_channels - f)\n",
    "    spec[:, f_zero:f_zero+f, :] = 0\n",
    "    return spec\n",
    "\n",
    "# Applies a time mask to a spectrogram\n",
    "def time_mask(spec, T=40):\n",
    "    spec_len = spec.shape[2]\n",
    "    t = random.randrange(0, T)\n",
    "    t_zero = random.randrange(0, spec_len - t)\n",
    "    spec[:, :, t_zero:t_zero+t] = 0\n",
    "    return spec\n",
    "\n",
    "# Taken from https://www.earthinversion.com/datascience/pink_noise_vs_white_noise/\n",
    "def generate_pink_noise(samples):\n",
    "    b = [0.049922035, -0.095993537, 0.050612699, -0.004408786]\n",
    "    a = [1, -2.494956002, 2.017265875, -0.522189400]\n",
    "\n",
    "    pink_noise = np.random.randn(samples)\n",
    "    pink_noise = np.convolve(pink_noise, b)\n",
    "    pink_noise = np.convolve(pink_noise, a, mode='valid')\n",
    "    return torch.Tensor(pink_noise).unsqueeze(0)\n",
    "\n",
    "# Signal is a torch tensor of shape [1, _]\n",
    "# snr is signal to noise ratio in dB, a float\n",
    "def add_pink_noise(signal, snr):\n",
    "    pink = generate_pink_noise(signal.shape[1])\n",
    "    return torchaudio.functional.add_noise(signal, pink, torch.Tensor([snr]))\n",
    "\n",
    "# def add_bg_noise(signal, snr):\n",
    "#    bg_noise = nocalls['signal'].sample(1).iloc[0]\n",
    "#    return torchaudio.functional.add_noise(signal, bg_noise, torch.Tensor([snr]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# IMPORT BIRDCALL DETECTION MODEL\n",
    "################################################################################\n",
    "\n",
    "\n",
    "class BirdCallDetector(nn.Module):\n",
    "    ''' Full architecture from https://github.com/musikalkemist/pytorchforaudio/blob/main/10%20Predictions%20with%20sound%20classifier/cnn.py'''\n",
    "    def __init__(self):\n",
    "        super(BirdCallDetector, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=16,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=16,\n",
    "                out_channels=32,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=128,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=128,\n",
    "                out_channels=128,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv6 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=128,\n",
    "                out_channels=128,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(10368, 2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    def forward(self, input_data):\n",
    "        x = self.conv1(input_data)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear(x)\n",
    "        predictions = self.softmax(logits)\n",
    "        return predictions\n",
    "\n",
    "# Set device we'll train on    \n",
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"Using {device}\")\n",
    "\n",
    "detection_model = BirdCallDetector().to(device)\n",
    "detection_model.load_state_dict(torch.load(CHECKPOINT_DIR + DETECTION_MODEL + '/final.pt', map_location=torch.device(device)))\n",
    "detection_model.eval()\n",
    "\n",
    "################################################################################\n",
    "################################################################################\n",
    "################################################################################\n",
    "################################################################################\n",
    "################################################################################\n",
    "################################################################################\n",
    "################################################################################\n",
    "################################################################################\n",
    "################################################################################\n",
    "################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# DATASET CLASS\n",
    "################################################################################\n",
    "\n",
    "# Slice a sequence into segments\n",
    "def slices(seq, window_size = SAMPLE_RATE*SAMPLE_LENGTH, stride = None, align_left = True):\n",
    "    # If one window is larger than the sequence, just return the scraps or nothing\n",
    "    if window_size > seq.shape[0]:\n",
    "        if seq.shape[0] < SAMPLE_RATE*MIN_SAMPLE_LENGTH:\n",
    "            return []\n",
    "        pad_length = SAMPLE_RATE*SAMPLE_LENGTH - seq.shape[0]\n",
    "        if align_left == False:\n",
    "            seq = torch.flip(seq, [0])\n",
    "            seq = torch.nn.functional.pad(seq, (0, pad_length))\n",
    "            return [torch.flip(seq, [0])]\n",
    "        return [torch.nn.functional.pad(seq, (0, pad_length))]\n",
    "        \n",
    "    # If stride is None, it defaults to window_size\n",
    "    if stride == None:\n",
    "        stride = window_size\n",
    "\n",
    "    index_slices = []\n",
    "    left_pointer = 0\n",
    "    while left_pointer + window_size <= seq.shape[0]:\n",
    "        index_slices += [[left_pointer, left_pointer + window_size]]\n",
    "        left_pointer += stride\n",
    "\n",
    "    if align_left == False:\n",
    "        offset = seq.shape[0]-(left_pointer-stride)-window_size\n",
    "        index_slices = [[a+offset, b+offset] for [a,b] in index_slices]\n",
    "\n",
    "    result = [seq[a:b] for [a,b] in index_slices]\n",
    "\n",
    "    if align_left == True and left_pointer < seq.shape[0]:\n",
    "        scrap = seq[left_pointer : seq.shape[0]]\n",
    "        pad_length = window_size - (seq.shape[0]-left_pointer)\n",
    "        scrap = torch.nn.functional.pad(scrap, (0, pad_length))\n",
    "        result.append(scrap)\n",
    "\n",
    "    elif align_left == False and index_slices[0][1] > stride:\n",
    "        scrap = seq[0: index_slices[0][1]-stride]\n",
    "        scrap = torch.flip(scrap, [0])\n",
    "        pad_length = window_size - (index_slices[0][1]-stride)\n",
    "        scrap = torch.nn.functional.pad(scrap, (0, pad_length))\n",
    "        result = [torch.flip(scrap, [0])] + result\n",
    "    \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: signals and labels should be ordinary lists\n",
    "# Training boolean is used to decide whether to apply masks\n",
    "# Config should have the format of a dictionary\n",
    "class BirdDataset(Dataset):\n",
    "    def __init__(self, signals, labels, \n",
    "                 training = True,\n",
    "                 use_mel = True,\n",
    "                 use_time_mask = True, \n",
    "                 use_freq_mask = True, \n",
    "                 use_pink_noise = True):\n",
    "        super().__init__()\n",
    "        self.training = training\n",
    "        self.use_mel = use_mel\n",
    "        self.use_time_mask = use_time_mask\n",
    "        self.use_freq_mask = use_freq_mask\n",
    "        self.use_pink_noise = use_pink_noise\n",
    "        print(f'Preprocessing {\"training\" if training else \"validation\"} data\\n')\n",
    "        self.processed_clips, self.labels = self.process(signals, labels)\n",
    "\n",
    "    def process(self, signals, labels):\n",
    "        processed_clips = []\n",
    "        processed_labels = []\n",
    "        for i, signal in enumerate(tqdm(signals, total = len(signals), leave = False)):\n",
    "            # Cut signal into 5 second chunks and process each clip separately\n",
    "            clips = slices(signal.squeeze())\n",
    "            clip_labels = []\n",
    "\n",
    "            for clip in clips:\n",
    "                x = clip.unsqueeze(0)\n",
    "                x = spectrogram_transform(x)\n",
    "                x = mel_spectrogram_transform(x)\n",
    "                x = db_scaler(x)\n",
    "                x = resize(x)\n",
    "                x = x.to(device)\n",
    "                prob_birdcall = detection_model(x.unsqueeze(0))[0][1].item()\n",
    "                if prob_birdcall >= 0.5:\n",
    "                    clip_labels += [labels[i]]\n",
    "                else:\n",
    "                    clip_labels += [torch.zeros(len(species))]\n",
    "            if len(clips) > 0:\n",
    "                processed_labels += [clip_labels]\n",
    "                processed_clips += [[x.unsqueeze(0) for x in clips]]\n",
    "\n",
    "        return processed_clips, processed_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.processed_clips)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        segments = self.processed_clips[index]\n",
    "        segment_labels = self.labels[index]\n",
    "\n",
    "        segment_index = np.random.randint(low = 0, high = len(segments))\n",
    "\n",
    "        x = segments[segment_index]\n",
    "        label = segment_labels[segment_index]\n",
    "\n",
    "        # Add pink noise\n",
    "        if self.use_pink_noise and self.training:\n",
    "            x = add_pink_noise(x, np.random.uniform(low = MIN_SNR, high = MAX_SNR))\n",
    "\n",
    "        # Process\n",
    "        x = spectrogram_transform(x)\n",
    "        if self.use_mel:\n",
    "            x = mel_spectrogram_transform(x)\n",
    "        if self.training:\n",
    "            exponent = np.random.uniform(low = 0.5, high = 3)\n",
    "            x = torch.pow(x, exponent)\n",
    "        x = db_scaler(x)\n",
    "        if self.use_time_mask and self.training:\n",
    "            x = time_mask(x)\n",
    "        if self.use_freq_mask and self.training:\n",
    "            x = freq_mask(x)\n",
    "        x = resize(x)\n",
    "        return x, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# ARCHITECTURE\n",
    "################################################################################\n",
    "\n",
    "\n",
    "class BirdClassifier(nn.Module):\n",
    "    ''' Full architecture from https://github.com/musikalkemist/pytorchforaudio/blob/main/10%20Predictions%20with%20sound%20classifier/cnn.py'''\n",
    "    def __init__(self, num_classes):\n",
    "        super(BirdClassifier, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=16,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=16,\n",
    "                out_channels=32,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=128,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=128,\n",
    "                out_channels=128,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout()\n",
    "        )\n",
    "        self.conv6 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=128,\n",
    "                out_channels=128,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout()\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(46208, num_classes)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        x = self.conv1(input_data)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# WEIGHTED RANDOM SAMPLER SETUP\n",
    "################################################################################\n",
    "\n",
    "#label_counts = data_train['primary_label'].value_counts()\n",
    "\n",
    "#total_durs = data_train.groupby(by='primary_label')['duration'].sum()\n",
    "#total_durs = data_train['primary_label'].apply(lambda x: total_durs[x])\n",
    "\n",
    "#weights = data_train['primary_label'].apply(lambda x: 1/label_counts.loc[x])*(data_train['duration']/total_durs)\n",
    "#weights = data_train['duration']/total_durs\n",
    "#weighted_sampler = torch.utils.data.WeightedRandomSampler(weights.to_list(), len(weights))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# TRAINING SETUP\n",
    "################################################################################\n",
    "\n",
    "# Create a saving directory if needed\n",
    "output_dir = Path(f'{CHECKPOINT_DIR}{MODEL_NAME}')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_dir = f'{CHECKPOINT_DIR}{MODEL_NAME}'\n",
    "\n",
    "# Instantiate our training dataset\n",
    "train_dataset = BirdDataset(signals = data_train['signal'].to_list(), \n",
    "                            labels = data_train['tensor_label'].to_list(),\n",
    "                            training = True,\n",
    "                            use_pink_noise = True)\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=True)\n",
    "\n",
    "# Instantiate our validation dataset\n",
    "validation_dataset =  BirdDataset(signals = data_validation['signal'].to_list(), \n",
    "                                  labels = data_validation['tensor_label'].to_list(),\n",
    "                                  training = False)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Instantiate our model\n",
    "model = BirdClassifier(NUM_SPECIES).to(device)\n",
    "\n",
    "# Set our loss function and optimizer\n",
    "pos_weight = torch.ones([len(species)]).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(pos_weight=pos_weight).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
    "\n",
    "# Training loop\n",
    "print(f\"Training on {len(train_dataset)} samples with {BATCH_SIZE} samples per batch.\")\n",
    "if REPORT_VALIDATION_LOSS_PER_EPOCH == True:\n",
    "    print(f\"Validating on {len(validation_dataset)} samples at the end of each epoch.\")\n",
    "\n",
    "training_losses = [None]*NUM_EPOCHS\n",
    "validation_losses = [None]*NUM_EPOCHS\n",
    "\n",
    "torch.enable_grad() # Turn on the gradient\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# SAVE AND REPORT train test split\n",
    "################################################################################\n",
    "\n",
    "# Save train test split\n",
    "data_train[['primary_label', 'filepath']].to_csv(f\"{output_dir}/data_train.csv\", index = False)\n",
    "data_validation[['primary_label', 'filepath']].to_csv(f\"{output_dir}/data_validation.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TRAINING LOOP\n",
    "################################################################################\n",
    "\n",
    "for epoch_num, epoch in enumerate(tqdm(range(NUM_EPOCHS), leave = False)):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(tqdm(train_dataloader, leave = False)):\n",
    "        \n",
    "        # Get batch of inputs and true labels, push to device\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass on batch of inputs\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Save checkpoint\n",
    "    if SAVE_CHECKPOINTS == True:\n",
    "        torch.save(model.state_dict(), f\"{CHECKPOINT_DIR}{MODEL_NAME}/checkpoint_{epoch_num+1}.pt\")\n",
    "\n",
    "    # Compute training loss\n",
    "    if REPORT_TRAINING_LOSS_PER_EPOCH == True:    \n",
    "        training_losses[epoch_num] = running_loss/len(train_dataloader)\n",
    "        \n",
    "    # Compute validation loss\n",
    "    if REPORT_VALIDATION_LOSS_PER_EPOCH == True:\n",
    "        validation_loss = 0.0\n",
    "        model.eval()\n",
    "        torch.no_grad()\n",
    "        for validation_data in validation_dataloader:\n",
    "            inputs, labels = validation_data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            validation_loss += criterion(outputs, labels).item()\n",
    "        validation_losses[epoch_num] = validation_loss/len(validation_dataloader)\n",
    "        model.train()\n",
    "        torch.enable_grad()\n",
    "\n",
    "    # Save losses\n",
    "    losses = pd.DataFrame({\"training_losses\":training_losses, \"validation_losses\":validation_losses})\n",
    "    cols = []\n",
    "    if REPORT_TRAINING_LOSS_PER_EPOCH == True:\n",
    "        cols += [\"training_losses\"]\n",
    "    if REPORT_VALIDATION_LOSS_PER_EPOCH == True:\n",
    "        cols += [\"validation_losses\"]\n",
    "    if len(cols) > 0:\n",
    "        losses[cols].to_csv(f'{output_dir}/losses.csv', index = False)\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# SAVE AND REPORT \n",
    "################################################################################\n",
    "\n",
    "# Save model\n",
    "if SAVE_AFTER_TRAINING == True:\n",
    "    torch.save(model.state_dict(), f'{output_dir}/final.pt')\n",
    "\n",
    "################################################################################\n",
    "################################################################################\n",
    "################################################################################\n",
    "################################################################################\n",
    "################################################################################\n",
    "################################################################################\n",
    "################################################################################\n",
    "################################################################################\n",
    "################################################################################\n",
    "################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
