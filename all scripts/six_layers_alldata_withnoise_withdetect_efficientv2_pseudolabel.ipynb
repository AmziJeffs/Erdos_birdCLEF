{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing modules\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# SETUP\n",
    "################################################################################\n",
    "\n",
    "# Convenience and saving flags\n",
    "ABRIDGED_RUN = True\n",
    "SAVE_AFTER_TRAINING = True # Save the model when you are done\n",
    "SAVE_CHECKPOINTS = True # Save the model after ever epoch\n",
    "REPORT_TRAINING_LOSS_PER_EPOCH = True # Track the training loss each epoch, and write it to a file after training\n",
    "REPORT_VALIDATION_LOSS_PER_EPOCH = True # Lets us make a nice learning curve after training\n",
    "\n",
    "# Training hyperparameters\n",
    "BATCH_SIZE = 96 # Number of samples per batch while training our network\n",
    "NUM_EPOCHS = 10 # Number of epochs to train our network\n",
    "LEARNING_RATE = 0.001 # Learning rate for our optimizer\n",
    "\n",
    "# Directories\n",
    "CHECKPOINT_DIR = \"checkpoints/\" # Checkpoints, models, and training data will be saved here\n",
    "DATA_DIR = \"../data/\"\n",
    "AUDIO_DIR = DATA_DIR + \"train_audio/\"\n",
    "UNLABELED_DIR = DATA_DIR + \"unlabeled_soundscapes/\"\n",
    "\n",
    "# CURRENT PIPELINE:\n",
    "#  - Split each audio file into 5 second clips\n",
    "#  - Discard any scrap with duration less than 3.5s. Pad others.\n",
    "#  - Run Birdcall detection on each clip and change labels appropriately\n",
    "#  - Loss function is CrossEntropyLoss\n",
    "#  - Train with freq/time masking, random power, and pink bg noise.\n",
    "#  - Validate without freq/time masking, random power, or bg noise.\n",
    "#  - Model output will be a vector of logits. Need to apply sigmoid to get probabilities.\n",
    "\n",
    "\n",
    "MODEL_NAME = \"CE_ALLDATA_PINKBG_WITHDETECT_RANDOMSAMPLE_PSEUDOLABEL\"\n",
    "DETECTION_MODEL = 'BIRDCALL_DETECTION_DCASE_FINAL'\n",
    "\n",
    "# Preprocessing info\n",
    "SAMPLE_RATE = 32000 # All our audio uses this sample rate\n",
    "SAMPLE_LENGTH = 5 # Duration we want to crop our audio to\n",
    "NUM_SPECIES = 182 # Number of bird species we need to label\n",
    "MIN_SAMPLE_LENGTH = 3.5 # Only use samples with length >= 3.5 seconds\n",
    "\n",
    "# Min and max signal to noise ratio\n",
    "MAX_SNR = 20\n",
    "MIN_SNR = 10\n",
    "\n",
    "################################################################################\n",
    "# IMPORTS\n",
    "################################################################################\n",
    "\n",
    "print(\"Importing modules\")\n",
    "\n",
    "# Basic imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import librosa\n",
    "import random\n",
    "import os\n",
    "import IPython.display as ipd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Torch imports\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchaudio.transforms import MelSpectrogram, Resample\n",
    "from IPython.display import Audio\n",
    "import torch.optim as optim\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# LOAD DATA\n",
    "################################################################################\n",
    "\n",
    "data = pd.read_csv(DATA_DIR+\"full_metadata.csv\")\n",
    "data['filepath'] = AUDIO_DIR + data['filename']\n",
    "\n",
    "# We only need the filepath, primary label, and duration(?)\n",
    "data = data[['filepath', 'primary_label', 'duration']]\n",
    "\n",
    "# Replace string labels by tensors whose entries are dummies\n",
    "species = ['asbfly', 'ashdro1', 'ashpri1', 'ashwoo2', 'asikoe2', 'asiope1', 'aspfly1', 'aspswi1', 'barfly1', 'barswa', 'bcnher', 'bkcbul1', 'bkrfla1', 'bkskit1', 'bkwsti', 'bladro1', 'blaeag1', 'blakit1', 'blhori1', 'blnmon1', 'blrwar1', 'bncwoo3', 'brakit1', 'brasta1', 'brcful1', 'brfowl1', 'brnhao1', 'brnshr', 'brodro1', 'brwjac1', 'brwowl1', 'btbeat1', 'bwfshr1', 'categr', 'chbeat1', 'cohcuc1', 'comfla1', 'comgre', 'comior1', 'comkin1', 'commoo3', 'commyn', 'compea', 'comros', 'comsan', 'comtai1', 'copbar1', 'crbsun2', 'cregos1', 'crfbar1', 'crseag1', 'dafbab1', 'darter2', 'eaywag1', 'emedov2', 'eucdov', 'eurbla2', 'eurcoo', 'forwag1', 'gargan', 'gloibi', 'goflea1', 'graher1', 'grbeat1', 'grecou1', 'greegr', 'grefla1', 'grehor1', 'grejun2', 'grenig1', 'grewar3', 'grnsan', 'grnwar1', 'grtdro1', 'gryfra', 'grynig2', 'grywag', 'gybpri1', 'gyhcaf1', 'heswoo1', 'hoopoe', 'houcro1', 'houspa', 'inbrob1', 'indpit1', 'indrob1', 'indrol2', 'indtit1', 'ingori1', 'inpher1', 'insbab1', 'insowl1', 'integr', 'isbduc1', 'jerbus2', 'junbab2', 'junmyn1', 'junowl1', 'kenplo1', 'kerlau2', 'labcro1', 'laudov1', 'lblwar1', 'lesyel1', 'lewduc1', 'lirplo', 'litegr', 'litgre1', 'litspi1', 'litswi1', 'lobsun2', 'maghor2', 'malpar1', 'maltro1', 'malwoo1', 'marsan', 'mawthr1', 'moipig1', 'nilfly2', 'niwpig1', 'nutman', 'orihob2', 'oripip1', 'pabflo1', 'paisto1', 'piebus1', 'piekin1', 'placuc3', 'plaflo1', 'plapri1', 'plhpar1', 'pomgrp2', 'purher1', 'pursun3', 'pursun4', 'purswa3', 'putbab1', 'redspu1', 'rerswa1', 'revbul', 'rewbul', 'rewlap1', 'rocpig', 'rorpar', 'rossta2', 'rufbab3', 'ruftre2', 'rufwoo2', 'rutfly6', 'sbeowl1', 'scamin3', 'shikra1', 'smamin1', 'sohmyn1', 'spepic1', 'spodov', 'spoowl1', 'sqtbul1', 'stbkin1', 'sttwoo1', 'thbwar1', 'tibfly3', 'tilwar1', 'vefnut1', 'vehpar1', 'wbbfly1', 'wemhar1', 'whbbul2', 'whbsho3', 'whbtre1', 'whbwag1', 'whbwat1', 'whbwoo2', 'whcbar1', 'whiter2', 'whrmun', 'whtkin2', 'woosan', 'wynlau1', 'yebbab1', 'yebbul3', 'zitcis1']\n",
    "\n",
    "species_to_index = {species[i]:i for i in range(len(species))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# GENERATING PSEUDOLABELS ON UNLABELED DATA VIA TRANSFER LEARNING \n",
    "################################################################################\n",
    "\n",
    "# Using ideas from \"Transfer Learning with Pseudo Multi-Label Birdcall Classification for DS@GT BirdCLEF 2024\" \n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "google_model = hub.load('https://www.kaggle.com/models/google/bird-vocalization-classifier/TensorFlow2/bird-vocalization-classifier/8')\n",
    "google_labels_path = hub.resolve('https://kaggle.com/models/google/bird-vocalization-classifier/frameworks/tensorFlow2/variations/bird-vocalization-classifier/versions/8') + \"/assets/label.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_labels = pd.read_csv(google_labels_path)\n",
    "labels_2021 = list(google_labels['ebird2021'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.zeros(len(labels_2021))\n",
    "\n",
    "for s in species:\n",
    "    if s in labels_2021:\n",
    "        index = google_labels.index[google_labels['ebird2021'] == s].tolist()[0]\n",
    "        mask[index] = 1\n",
    "\n",
    "mask = np.array(mask, dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-16.182888 -10.089521 -17.00353  ... -18.126009 -15.91088  -13.197889]\n",
      "[-12.140304    -9.536605   -15.159098   -17.414268    -9.26193\n",
      " -18.59745    -11.652218   -15.857957   -10.374762   -11.004333\n",
      "  -8.6138115  -12.852977   -13.015774   -10.015996   -10.748857\n",
      " -12.932539   -10.830873    -8.7248955   -8.197229   -11.206668\n",
      " -14.371604   -17.546017   -16.054077    -7.0142694  -13.986792\n",
      " -11.325673   -14.691146    -8.70832    -17.267355   -10.21945\n",
      " -16.227596   -12.683728   -12.275227   -12.37685    -14.677467\n",
      " -12.145354    -8.851334    -9.397553    -8.705748    -5.918978\n",
      " -10.918662   -10.351061   -10.846949    -7.2676044   -8.825595\n",
      " -10.443416    -9.847268   -14.213484   -13.613717    -9.316313\n",
      "  -9.201786   -14.071221    -8.616915   -10.9564905   -9.4638\n",
      " -11.377034    -7.354533   -12.737599   -11.398599   -15.457252\n",
      " -10.872189    -9.095661   -12.981686    -8.135442   -11.680374\n",
      "  -9.372511   -10.965663    -5.415431   -15.417003    -8.822309\n",
      "  -8.913292   -11.486623   -10.766799   -15.352887   -16.855148\n",
      "  -8.395283   -12.795004   -10.028631   -14.26538    -10.456776\n",
      " -10.668439    -9.657193   -13.859837    -6.3040705  -14.415416\n",
      " -15.25736    -11.809511   -12.881777    -6.4020867  -13.056467\n",
      " -14.619393   -18.470238   -17.774834   -13.681845   -19.484241\n",
      " -10.036124   -11.116107   -16.34309    -11.768322   -10.909054\n",
      "  -9.151655   -12.635808   -17.338614    -7.3219814  -11.134024\n",
      "  -7.19568     -8.937774   -11.355834   -15.295865    -7.60101\n",
      " -12.369923    -9.254778    -7.471884   -14.005184    -9.476997\n",
      "  -7.609536   -11.558132    -4.6715117  -15.064608   -13.635857\n",
      " -16.354822   -13.33328    -17.378742   -13.121602   -12.576404\n",
      " -10.840228   -10.262959   -10.092129   -12.114088   -12.746947\n",
      " -10.153904   -11.080677   -13.482806   -14.431628    -6.3823814\n",
      " -11.466114   -11.527383   -14.108863    -9.908135   -14.919543\n",
      " -10.048015   -11.393833   -15.164338   -14.067778    -7.3382087\n",
      " -10.571702   -17.329885   -10.3250265  -12.88111    -10.779806\n",
      " -17.104715    -9.790498    -9.682199    -7.0725884  -17.616474\n",
      "  -8.278719    -9.503116   -11.102245   -16.55117     -9.143182\n",
      " -10.187753   -14.204794   -14.204881   -15.857726    -9.453457\n",
      " -11.539283    -8.822888    -9.35862    -15.53593    -13.950805\n",
      "  -9.596293    -0.42804044 -14.9913225  -14.370516   -10.973956\n",
      "  -8.847137   -15.324857   -13.404436   -10.170381   -13.550943  ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-15.348285 -10.758653 -15.321203 ... -16.893723 -15.967014 -13.763664]\n",
      "[-11.362393   -9.558344  -13.970017  -17.353964   -8.313494  -16.743874\n",
      " -10.6454115 -13.265311   -7.6191697 -11.808588   -7.161933   -8.44035\n",
      " -13.819398  -10.384457  -11.179996  -12.011367   -7.5895534  -8.927334\n",
      "  -8.056838  -12.481757  -13.965495  -17.31228   -17.206177   -6.1871696\n",
      " -13.101134  -11.941836  -14.138417   -7.4883566 -17.05321   -11.049992\n",
      " -15.497989  -11.447145  -13.559957  -11.561776  -12.564599  -11.320187\n",
      "  -8.507873   -9.7153425  -9.507276   -5.5713296  -9.192698   -9.944891\n",
      "  -9.856054   -8.381234   -9.93821    -8.650188   -9.108253  -13.561369\n",
      " -11.786072   -7.9885683  -8.0727825 -12.960936   -8.465901   -6.479889\n",
      "  -9.392046   -7.7102714  -6.453208  -12.85343   -10.616078  -15.317446\n",
      " -11.236885   -9.173137  -13.192703   -8.760444  -10.575289  -10.913146\n",
      "  -8.920128   -2.4304976 -14.417489   -9.002702   -9.223293  -11.064366\n",
      "  -9.0742    -15.117284  -15.0643425  -8.508969  -13.655769   -9.392183\n",
      " -11.733763  -10.166887  -11.410827   -9.325625  -14.049373   -3.8629754\n",
      " -15.282566  -14.245235  -10.732409  -11.958776   -5.8192053 -13.028651\n",
      " -15.534823  -15.280849  -17.793129  -13.113279  -16.159863   -9.944277\n",
      "  -9.911488  -12.451995  -11.2154    -10.77163    -9.20261   -10.13321\n",
      " -19.282572   -8.819706  -10.50429    -6.5227847  -9.203858  -12.281019\n",
      " -14.667549   -7.728388  -12.582979   -6.5885267  -6.936561  -13.762275\n",
      "  -9.734717   -5.426962   -9.9692     -2.8829756 -12.8876915 -10.491132\n",
      " -17.249676  -11.438655  -17.826925  -12.618521  -11.979347   -8.643843\n",
      "  -7.7850595  -9.883309  -10.017919   -9.72655    -9.234404  -12.71376\n",
      " -12.034944  -14.317078   -8.277767  -10.35846   -13.159947  -10.96188\n",
      "  -8.167571  -15.868147  -10.070483  -11.756501  -16.378447  -10.87825\n",
      "  -7.1849494 -11.028726  -15.432169  -13.929161  -11.501363   -8.70476\n",
      " -16.137203   -7.2480693  -9.876195   -7.3539724 -15.051188   -8.436845\n",
      "  -7.8851967  -9.527934  -15.256759   -8.9558     -9.240835  -11.11918\n",
      "  -7.87407   -12.780577   -9.210207  -13.037749   -8.09826    -8.387603\n",
      " -13.701572  -11.362107  -11.347081   -1.9730941 -15.343894  -14.305668\n",
      " -13.03714    -8.684159  -12.47688   -13.673864   -5.0855727 -12.768049 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-17.05757  -11.525559 -18.700148 ... -18.578142 -19.21122  -14.377315]\n",
      "[-11.974232  -10.812713  -16.04171   -19.453636  -10.123717  -19.64402\n",
      " -11.742747  -16.066809   -8.526633  -10.957246   -7.7391386  -6.7066426\n",
      " -16.15545   -11.080449  -11.612846  -14.9026575  -9.718093  -10.006782\n",
      " -10.19087   -12.087269  -15.326747  -19.598902  -17.403471   -7.265856\n",
      " -14.3763685 -14.016483  -15.930182   -9.311806  -18.862942  -11.165615\n",
      " -18.251123  -14.215908  -14.174619  -11.987307  -13.604885  -15.515032\n",
      "  -7.6839266  -9.600037   -8.817249   -6.549249  -11.258695  -12.264466\n",
      "  -9.786207   -7.7936163 -12.096609  -11.78238   -11.25362   -17.855026\n",
      " -14.106731   -8.931691   -8.649327  -17.91407    -7.6642222 -11.218516\n",
      "  -9.948968  -10.3487835  -6.139553  -13.549328  -12.132354  -17.398655\n",
      " -11.782656  -10.150867  -14.010688  -10.449619  -12.001618  -10.964161\n",
      " -11.048334   -3.9125264 -13.340461   -9.61997    -7.8241987 -13.28901\n",
      "  -9.63088   -14.874231  -17.196035   -8.878504  -14.826193  -11.6880665\n",
      " -13.641824  -10.4941025 -13.0452585  -9.359991  -15.409558   -6.112065\n",
      " -15.3728    -16.625402  -12.496626  -15.7391205  -8.086805  -15.43646\n",
      " -16.772984  -16.445189  -20.154652  -13.946899  -19.121504  -10.084212\n",
      " -10.8910885 -14.286895  -13.019687  -13.019449  -12.815794  -12.340458\n",
      " -19.95332    -8.96479   -10.790774   -5.751111   -9.9352665 -14.763027\n",
      " -15.750189   -7.84258   -14.04432    -9.1461935 -11.490437  -13.834501\n",
      "  -9.511631   -8.210513  -11.991602   -5.187341  -15.301851  -13.929447\n",
      " -16.728323  -12.225732  -20.161135  -13.793859  -14.468936  -10.843808\n",
      " -10.426097  -12.021305  -12.20596   -10.756008   -9.908461  -14.379795\n",
      " -13.808894  -16.27938    -6.14369   -12.21466   -11.25544   -14.966053\n",
      "  -8.488145  -15.666566  -12.981669  -11.819812  -16.515844  -12.341579\n",
      "  -6.676     -15.248001  -17.284603  -13.531245  -12.679979  -12.551956\n",
      " -18.791395   -6.8942766 -12.4196415  -9.847326  -17.532963   -9.8967285\n",
      " -10.2244215 -11.326304  -17.301958  -11.36051    -9.942767  -13.768295\n",
      " -10.223474  -16.657104  -10.494108  -18.076422  -10.405962  -10.380031\n",
      " -16.275442  -13.40758   -12.883344   -5.7567153 -16.46073   -15.425457\n",
      " -12.812254   -8.48362   -15.345392  -12.760882   -9.762837  -12.281551 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-15.601288 -11.209043 -16.894306 ... -16.638704 -15.708735  -9.711019]\n",
      "[ -8.152239  -10.693705  -13.965099  -17.63632    -9.755977  -17.355137\n",
      "  -9.594298  -13.648219   -8.155355  -11.352509   -7.8667264  -5.5421863\n",
      " -14.826646  -11.285253  -12.600666  -11.107434  -10.0405245  -8.117164\n",
      "  -8.741984  -11.68797   -12.295974  -15.929054  -16.79174    -5.660163\n",
      " -12.898657  -11.072651  -14.516014   -7.542643  -18.389988  -11.286757\n",
      " -14.045475  -12.615016  -13.387349   -9.69118   -12.234995  -12.674728\n",
      "  -7.6159906  -9.209259   -7.6033382  -6.3603477  -9.081952  -10.371889\n",
      "  -9.757185   -7.53646    -7.8187833  -8.12741    -8.612406  -14.044363\n",
      " -11.807469   -7.8099437  -8.723592  -14.095173   -8.496742  -10.041669\n",
      "  -9.534459   -8.390752   -7.382246  -12.458094  -11.494764  -14.904117\n",
      " -10.636522   -9.112879  -12.296346   -9.310435  -12.431448  -10.2880335\n",
      "  -9.415657   -2.4976995 -14.232031   -7.1368265  -8.43322   -10.632404\n",
      "  -8.2008095 -13.728159  -16.199354   -8.949969  -12.761793  -10.10462\n",
      " -11.98822    -9.936453  -11.386923   -8.074144  -11.807518   -3.7451882\n",
      " -14.941202  -15.077695  -10.021828  -13.16784    -4.12255   -12.349028\n",
      " -15.797891  -14.774015  -20.574038  -11.683527  -14.95689    -8.530461\n",
      " -11.8544445 -12.099259  -10.8151    -11.919673  -10.093243   -9.623687\n",
      " -18.640648   -7.546285  -11.501599   -6.2879224  -8.2020645 -12.493184\n",
      " -14.198026   -6.589175  -10.195743   -7.0777617  -6.704494  -14.049463\n",
      "  -6.7997723  -6.419937   -8.97794    -2.6560056 -13.299467  -11.721324\n",
      " -15.352326  -10.197919  -18.86063   -13.890999  -12.617445   -8.775175\n",
      "  -7.398339  -10.184041  -10.556129  -10.000896  -10.769127  -10.698395\n",
      " -12.778456  -14.151672   -7.150846   -7.882698  -10.988833  -10.163757\n",
      "  -4.3816886 -14.491483  -10.182164   -9.854786  -15.991942  -11.1651\n",
      "  -4.9363537 -12.850818  -17.086523  -11.284036  -12.647857   -9.387698\n",
      " -16.432558   -6.1679974 -12.896932   -6.712926  -15.608245   -5.654174\n",
      "  -8.530565  -10.082641  -15.980645   -8.797408   -8.533622  -12.537553\n",
      "  -8.845681  -14.11726    -8.63134   -13.000389   -6.9252343  -7.7569475\n",
      " -12.909724  -11.310354  -11.118108   -1.5297834 -14.799054  -13.406144\n",
      " -11.074857   -8.898136  -13.02715   -10.710645   -8.237661  -12.696023 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-16.379719 -10.453501 -15.66096  ... -19.297249 -19.318588 -14.171504]\n",
      "[-11.391192  -10.560214  -14.719167  -19.053596  -10.519624  -16.571054\n",
      " -11.51387   -14.53553    -7.7954655 -11.354489   -8.950671   -8.873963\n",
      " -11.503669  -10.705521  -10.126854  -13.237711   -9.478981   -9.577209\n",
      "  -9.298851  -12.060806  -15.11545   -18.055782  -17.963432   -5.9903846\n",
      " -16.930641  -12.865107  -15.49391    -8.476657  -18.197115  -13.983986\n",
      " -15.544422  -11.410958  -13.065242  -11.107218  -14.364076  -12.501109\n",
      "  -8.183333   -8.244851   -9.453257   -5.990736  -10.362259  -10.70768\n",
      " -11.61987    -7.979307   -9.482979   -8.162142   -9.449031  -15.2501\n",
      " -13.017265   -7.503604   -9.1685095 -14.609624   -8.970085  -10.454048\n",
      "  -9.631452  -10.602648   -6.168769  -15.116363  -10.914024  -14.830975\n",
      " -12.637926  -10.077197  -12.014001  -10.274631  -12.44825   -11.385899\n",
      " -10.375581   -5.479348  -14.882657   -9.007939   -8.944242  -12.67725\n",
      "  -9.823525  -15.331909  -16.11495    -9.499203  -13.305759  -10.750429\n",
      " -11.730039  -10.929311  -12.6041155  -9.054416  -16.601564   -3.2717698\n",
      " -14.875403  -14.518822  -13.761327  -14.081312   -8.258365  -15.597783\n",
      " -17.217216  -17.527853  -18.418804  -13.1444    -18.583399  -11.103843\n",
      " -11.129288  -12.283532  -13.030659  -10.846987  -12.726328  -11.852114\n",
      " -19.891567   -8.323846  -11.966541   -7.5007915  -9.346188  -13.750162\n",
      " -13.025398   -7.6814685 -12.708856   -6.866227   -8.023025  -13.377118\n",
      " -10.951739   -8.668193  -10.557066   -5.507728  -13.746349  -11.456006\n",
      " -16.342161  -11.418753  -20.45687   -13.0094795 -11.846793   -9.825026\n",
      "  -7.4915323 -10.025805  -11.526764  -10.361586   -9.348408  -12.988905\n",
      " -13.6302185 -14.622235   -7.9416122 -10.596822  -12.84559   -12.557664\n",
      "  -7.84115   -15.952422  -11.028442  -10.217058  -17.149752  -12.667208\n",
      "  -6.856007  -12.704829  -18.159885  -14.318994  -13.257384   -9.430631\n",
      " -15.084353   -7.7522388 -11.674248   -7.896868  -16.289047   -9.453828\n",
      "  -9.058534   -9.31929   -18.642216  -10.675028   -9.437122  -11.553573\n",
      "  -9.989366  -15.179835   -8.6227255 -14.6493225  -8.146391  -10.661356\n",
      " -15.31863   -11.664926  -12.3866205  -2.7312257 -16.540085  -13.406349\n",
      " -10.949868  -10.850363  -13.141906  -10.406837   -7.3456645 -11.920646 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-15.635172  -9.196008 -16.463526 ... -18.175106 -14.552186 -11.667057]\n",
      "[-11.329766   -8.720218  -13.112525  -15.535604   -7.7091575 -17.288721\n",
      "  -9.359634  -14.304786   -7.4720345 -10.046458   -7.10174    -8.899191\n",
      " -12.134792  -10.086561   -9.993014  -14.148719   -9.949081   -7.934282\n",
      "  -8.876161  -10.4953575 -11.450548  -16.32067   -15.60618    -5.8343983\n",
      " -12.639773  -10.081034  -13.318407   -7.3055143 -15.170306   -9.087476\n",
      " -14.80859   -10.428169  -13.279226  -11.161552  -12.445295  -10.7390785\n",
      "  -7.8406296  -9.337217   -9.090274   -4.8488126  -8.368956   -9.919594\n",
      "  -7.8075824  -6.8842998  -7.244624   -9.112443   -8.132311  -13.96734\n",
      " -12.499348   -7.126862   -7.8725095 -13.593781   -8.010386  -10.793465\n",
      "  -9.283366  -11.591519   -6.1594763 -11.361806  -10.557654  -15.110458\n",
      "  -8.048146   -9.612197  -10.664937   -7.817873  -11.038181  -10.455446\n",
      " -11.027363   -2.9751415 -14.868645   -8.068899   -7.946876   -9.771543\n",
      "  -8.459249  -14.378116  -14.358719   -7.3341484 -11.945552   -9.198515\n",
      " -10.42036    -9.201027   -9.633624   -8.518703  -15.794004   -6.1211133\n",
      " -12.2051115 -12.981583  -10.017988  -10.34881    -6.6678405 -10.657355\n",
      " -12.985843  -14.705241  -16.054163  -14.723797  -17.207218   -6.7387915\n",
      "  -9.590901  -14.328941  -10.274594  -10.210487   -9.822125  -10.532979\n",
      " -18.286879   -6.999803   -9.805002   -5.2676716  -9.217864  -13.354051\n",
      " -13.757268   -6.0615606 -10.9321785  -7.487245   -7.099266  -11.523017\n",
      "  -8.103314   -5.8702393 -10.367621   -3.8498778 -13.678498  -12.597973\n",
      " -13.969244  -11.9995985 -16.057564  -11.910302  -12.040947   -8.695088\n",
      "  -6.4178686  -8.813427   -9.345654   -8.852037  -10.419202  -10.229812\n",
      " -11.175813  -13.620439   -7.1624537  -9.975755  -11.191372  -12.050739\n",
      "  -7.622698  -13.562297  -10.703383   -9.860743  -13.484952  -10.093533\n",
      "  -6.290119   -9.992717  -15.073366  -11.050796  -11.010027   -8.633762\n",
      " -14.769945   -7.611578   -9.312268   -7.023402  -15.41772    -7.413039\n",
      "  -7.9280105  -9.346231  -16.135094   -8.643945   -8.3954525  -9.308172\n",
      "  -8.620555  -13.089976   -8.353697  -11.606721   -7.6071286  -7.8831353\n",
      " -12.149535  -10.920356   -7.8248115  -1.5100839 -13.587043  -13.087714\n",
      "  -9.44783    -7.9062986 -11.840211  -11.567942   -7.6976495 -13.039629 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 6/48 [00:03<00:22,  1.86it/s]\n",
      "  0%|          | 0/10 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(num_segments)):\n\u001b[1;32m     22\u001b[0m     x \u001b[38;5;241m=\u001b[39m waveform[(step_size\u001b[38;5;241m*\u001b[39mi):(step_size\u001b[38;5;241m*\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))]\n\u001b[0;32m---> 23\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mgoogle_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_tf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnewaxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     logits \u001b[38;5;241m=\u001b[39m logits[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(logits)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1552\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1552\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1560\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1561\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1562\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1566\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1567\u001b[0m   )\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "unlabeled_soundscapes = sorted(os.listdir(UNLABELED_DIR))\n",
    "step_size = SAMPLE_RATE * 5\n",
    "count = 0 \n",
    "\n",
    "if ABRIDGED_RUN == True:\n",
    "    unlabeled_soundscapes = random.sample(unlabeled_soundscapes, 10)\n",
    "else:\n",
    "    unlabeled_soundscapes = random.sample(unlabeled_soundscapes, 800)\n",
    "    \n",
    "# for path in tqdm(unlabeled_soundscapes):\n",
    "#     num_segments = 240 // 5\n",
    "#     waveform, sr = librosa.load(UNLABELED_DIR + path, sr=SAMPLE_RATE)\n",
    "#     waveform = waveform.astype(np.float32)\n",
    "#     dur = librosa.get_duration(y=waveform, sr=sr)\n",
    "    \n",
    "    \n",
    "#     if len(waveform) < 240 * SAMPLE_RATE:\n",
    "#         continue\n",
    "    \n",
    "    \n",
    "#     for i in tqdm(range(num_segments)):\n",
    "#         x = waveform[(step_size*i):(step_size*(i+1))]\n",
    "#         logits = google_model.infer_tf(x[np.newaxis, :])\n",
    "#         logits = logits['label'].numpy()[0]\n",
    "#         logits = logits[mask]\n",
    "    \n",
    "#         pred_label = google_labels.iloc[[np.argmax(logits)]]['ebird2021'].item() \n",
    "        \n",
    "#         if pred_label in species: \n",
    "#             print(\"Adding new row to metadata\")\n",
    "#             count += 1\n",
    "#             new_row = {\n",
    "#             'filepath': UNLABELED_DIR + path,\n",
    "#             'primary_label': pred_label,\n",
    "#             'duration': dur\n",
    "#             }\n",
    "#             data = pd.concat([data, pd.DataFrame([new_row])])\n",
    "#             break \n",
    "    \n",
    "\n",
    "# print(f\"Added {count} datapoints from unlabeled soundscape\")\n",
    "\n",
    "for path in tqdm(unlabeled_soundscapes):\n",
    "    waveform, sr = librosa.load(UNLABELED_DIR + path, sr=SAMPLE_RATE)\n",
    "    waveform = waveform.astype(np.float32)\n",
    "    dur = librosa.get_duration(y=waveform, sr=sr)\n",
    "    \n",
    "    \n",
    "    if len(waveform) < 240 * SAMPLE_RATE:\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    for i in range(4):\n",
    "        start = np.random.randint(waveform.shape[0]-SAMPLE_RATE*SAMPLE_LENGTH+1)\n",
    "\n",
    "        x = waveform[start:start + SAMPLE_RATE*SAMPLE_LENGTH]\n",
    "        logits = google_model.infer_tf(x[np.newaxis, :])\n",
    "        logits = logits['label'].numpy()[0]\n",
    "        logits = logits[mask]\n",
    "        pred_label = google_labels.iloc[[np.argmax(logits)]]['ebird2021'].item() \n",
    "        \n",
    "        if pred_label in species: \n",
    "            print(\"Adding new row to metadata\")\n",
    "            count += 1\n",
    "            new_row = {\n",
    "            'filepath': UNLABELED_DIR + path,\n",
    "            'primary_label': pred_label,\n",
    "            'duration': dur\n",
    "            }\n",
    "\n",
    "        data = pd.concat([data, pd.DataFrame([new_row])])\n",
    "    \n",
    "\n",
    "\n",
    "print(f\"Added {count} datapoints from unlabeled soundscape\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['index_label'] = data['primary_label'].apply(lambda x: species_to_index[x])\n",
    "data['tensor_label'] = pd.Series(pd.get_dummies(data['primary_label']).astype(int).values.tolist()).apply(lambda x: torch.Tensor(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'afcdov1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex_label\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprimary_label\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mspecies_to_index\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensor_label\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(pd\u001b[38;5;241m.\u001b[39mget_dummies(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprimary_label\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist())\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: torch\u001b[38;5;241m.\u001b[39mTensor(x))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Remove overly short clips\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[105], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex_label\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprimary_label\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mspecies_to_index\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      2\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensor_label\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(pd\u001b[38;5;241m.\u001b[39mget_dummies(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprimary_label\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist())\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: torch\u001b[38;5;241m.\u001b[39mTensor(x))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Remove overly short clips\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'afcdov1'"
     ]
    }
   ],
   "source": [
    "# Remove overly short clips\n",
    "data = data[data['duration'] >= MIN_SAMPLE_LENGTH]\n",
    "\n",
    "# Use 100 rows of data for quick runs to test functionalities\n",
    "if ABRIDGED_RUN == True:\n",
    "    data = data.sample(100)\n",
    "\n",
    "# Progress bars for loading data to memory\n",
    "tqdm.pandas()\n",
    "\n",
    "# print(\"Loading nocall background noise snippets into memory\")\n",
    "# nocalls = pd.read_csv(f\"{DATA_DIR}nocall_snippets/filenames.csv\")\n",
    "# def load_nocall(filepath):\n",
    "#    signal, _ = torchaudio.load(filepath)\n",
    "    # Reduce to one channel\n",
    "#    signal = torch.mean(signal,dim = 0).unsqueeze(0)\n",
    "#    return signal\n",
    "# nocalls['signal'] = nocalls['filename'].progress_apply(lambda x: load_nocall(f\"{DATA_DIR}nocall_snippets/{x}\"))\n",
    "# print(\"Done\")\n",
    "\n",
    "print(\"Loading training audio signals into memory\")\n",
    "# Loads signal to memory, pads to 5 seconds, cuts to 60 seconds\n",
    "def filepath_to_signal(filepath):\n",
    "    signal, _ = torchaudio.load(filepath)\n",
    "    return signal\n",
    "data['signal'] = data['filepath'].progress_apply(filepath_to_signal)\n",
    "print(\"Done\")\n",
    "\n",
    "# Train test split, stratified by species\n",
    "stratify = data['primary_label']\n",
    "if ABRIDGED_RUN == True:\n",
    "    stratify = None\n",
    "data_train, data_validation = train_test_split(data, test_size = 0.2, stratify=stratify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# PREPROCESSING FUNCTIONS\n",
    "################################################################################\n",
    "\n",
    "# Transforms audio signal to a spectrogram\n",
    "spectrogram_transform = torchaudio.transforms.Spectrogram(\n",
    "        n_fft=2048,\n",
    "        win_length=2048,\n",
    "        hop_length=512,\n",
    "        power=2\n",
    "    )\n",
    "\n",
    "# Converts ordinary spectrogram to Mel scale\n",
    "mel_spectrogram_transform = torchaudio.transforms.MelScale(\n",
    "    n_mels=128,\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    f_min=40,\n",
    "    f_max=16000,\n",
    "    n_stft=1025  # the number of frequency bins in the spectrogram\n",
    ")\n",
    "\n",
    "# Scales decibels to reasonable level (apply to a spectrogram or Mel spectrogram)\n",
    "db_scaler = torchaudio.transforms.AmplitudeToDB(stype=\"power\", top_db=80)\n",
    "\n",
    "# Resizes spectrograms into square images\n",
    "resize = transforms.Resize((224, 224), antialias = None)\n",
    "\n",
    "# Applies a frequency mask to a spectrogram\n",
    "def freq_mask(spec, F=30):\n",
    "    num_mel_channels = spec.shape[1]\n",
    "    f = random.randrange(0, F)\n",
    "    f_zero = random.randrange(0, num_mel_channels - f)\n",
    "    spec[:, f_zero:f_zero+f, :] = 0\n",
    "    return spec\n",
    "\n",
    "# Applies a time mask to a spectrogram\n",
    "def time_mask(spec, T=40):\n",
    "    spec_len = spec.shape[2]\n",
    "    t = random.randrange(0, T)\n",
    "    t_zero = random.randrange(0, spec_len - t)\n",
    "    spec[:, :, t_zero:t_zero+t] = 0\n",
    "    return spec\n",
    "\n",
    "# Taken from https://www.earthinversion.com/datascience/pink_noise_vs_white_noise/\n",
    "def generate_pink_noise(samples):\n",
    "    b = [0.049922035, -0.095993537, 0.050612699, -0.004408786]\n",
    "    a = [1, -2.494956002, 2.017265875, -0.522189400]\n",
    "\n",
    "    pink_noise = np.random.randn(samples)\n",
    "    pink_noise = np.convolve(pink_noise, b)\n",
    "    pink_noise = np.convolve(pink_noise, a, mode='valid')\n",
    "    return torch.Tensor(pink_noise).unsqueeze(0)\n",
    "\n",
    "# Signal is a torch tensor of shape [1, _]\n",
    "# snr is signal to noise ratio in dB, a float\n",
    "def add_pink_noise(signal, snr):\n",
    "    pink = generate_pink_noise(signal.shape[1])\n",
    "    return torchaudio.functional.add_noise(signal, pink, torch.Tensor([snr]))\n",
    "\n",
    "# def add_bg_noise(signal, snr):\n",
    "#    bg_noise = nocalls['signal'].sample(1).iloc[0]\n",
    "#    return torchaudio.functional.add_noise(signal, bg_noise, torch.Tensor([snr]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# IMPORT BIRDCALL DETECTION MODEL\n",
    "################################################################################\n",
    "\n",
    "\n",
    "class BirdCallDetector(nn.Module):\n",
    "    ''' Full architecture from https://github.com/musikalkemist/pytorchforaudio/blob/main/10%20Predictions%20with%20sound%20classifier/cnn.py'''\n",
    "    def __init__(self):\n",
    "        super(BirdCallDetector, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=16,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=16,\n",
    "                out_channels=32,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=128,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=128,\n",
    "                out_channels=128,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv6 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=128,\n",
    "                out_channels=128,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(10368, 2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    def forward(self, input_data):\n",
    "        x = self.conv1(input_data)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear(x)\n",
    "        predictions = self.softmax(logits)\n",
    "        return predictions\n",
    "\n",
    "# Set device we'll train on    \n",
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"Using {device}\")\n",
    "\n",
    "detection_model = BirdCallDetector().to(device)\n",
    "detection_model.load_state_dict(torch.load(CHECKPOINT_DIR + DETECTION_MODEL + '/final.pt', map_location=torch.device(device)))\n",
    "detection_model.eval()\n",
    "\n",
    "################################################################################\n",
    "################################################################################\n",
    "################################################################################\n",
    "################################################################################\n",
    "################################################################################\n",
    "################################################################################\n",
    "################################################################################\n",
    "################################################################################\n",
    "################################################################################\n",
    "################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# DATASET CLASS\n",
    "################################################################################\n",
    "\n",
    "# Slice a sequence into segments\n",
    "def slices(seq, window_size = SAMPLE_RATE*SAMPLE_LENGTH, stride = None, align_left = True):\n",
    "    # If one window is larger than the sequence, just return the scraps or nothing\n",
    "    if window_size > seq.shape[0]:\n",
    "        if seq.shape[0] < SAMPLE_RATE*MIN_SAMPLE_LENGTH:\n",
    "            return []\n",
    "        pad_length = SAMPLE_RATE*SAMPLE_LENGTH - seq.shape[0]\n",
    "        if align_left == False:\n",
    "            seq = torch.flip(seq, [0])\n",
    "            seq = torch.nn.functional.pad(seq, (0, pad_length))\n",
    "            return [torch.flip(seq, [0])]\n",
    "        return [torch.nn.functional.pad(seq, (0, pad_length))]\n",
    "        \n",
    "    # If stride is None, it defaults to window_size\n",
    "    if stride == None:\n",
    "        stride = window_size\n",
    "\n",
    "    index_slices = []\n",
    "    left_pointer = 0\n",
    "    while left_pointer + window_size <= seq.shape[0]:\n",
    "        index_slices += [[left_pointer, left_pointer + window_size]]\n",
    "        left_pointer += stride\n",
    "\n",
    "    if align_left == False:\n",
    "        offset = seq.shape[0]-(left_pointer-stride)-window_size\n",
    "        index_slices = [[a+offset, b+offset] for [a,b] in index_slices]\n",
    "\n",
    "    result = [seq[a:b] for [a,b] in index_slices]\n",
    "\n",
    "    if align_left == True and left_pointer < seq.shape[0]:\n",
    "        scrap = seq[left_pointer : seq.shape[0]]\n",
    "        pad_length = window_size - (seq.shape[0]-left_pointer)\n",
    "        scrap = torch.nn.functional.pad(scrap, (0, pad_length))\n",
    "        result.append(scrap)\n",
    "\n",
    "    elif align_left == False and index_slices[0][1] > stride:\n",
    "        scrap = seq[0: index_slices[0][1]-stride]\n",
    "        scrap = torch.flip(scrap, [0])\n",
    "        pad_length = window_size - (index_slices[0][1]-stride)\n",
    "        scrap = torch.nn.functional.pad(scrap, (0, pad_length))\n",
    "        result = [torch.flip(scrap, [0])] + result\n",
    "    \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: signals and labels should be ordinary lists\n",
    "# Training boolean is used to decide whether to apply masks\n",
    "# Config should have the format of a dictionary\n",
    "class BirdDataset(Dataset):\n",
    "    def __init__(self, signals, labels, \n",
    "                 training = True,\n",
    "                 use_mel = True,\n",
    "                 use_time_mask = True, \n",
    "                 use_freq_mask = True, \n",
    "                 use_pink_noise = True):\n",
    "        super().__init__()\n",
    "        self.training = training\n",
    "        self.use_mel = use_mel\n",
    "        self.use_time_mask = use_time_mask\n",
    "        self.use_freq_mask = use_freq_mask\n",
    "        self.use_pink_noise = use_pink_noise\n",
    "        print(f'Preprocessing {\"training\" if training else \"validation\"} data\\n')\n",
    "        self.processed_clips, self.labels = self.process(signals, labels)\n",
    "\n",
    "    def process(self, signals, labels):\n",
    "        processed_clips = []\n",
    "        processed_labels = []\n",
    "        for i, signal in enumerate(tqdm(signals, total = len(signals), leave = False)):\n",
    "            # Cut signal into 5 second chunks and process each clip separately\n",
    "            clips = slices(signal.squeeze())\n",
    "            clip_labels = []\n",
    "\n",
    "            for clip in clips:\n",
    "                x = clip.unsqueeze(0)\n",
    "                x = spectrogram_transform(x)\n",
    "                x = mel_spectrogram_transform(x)\n",
    "                x = db_scaler(x)\n",
    "                x = resize(x)\n",
    "                x = x.to(device)\n",
    "                prob_birdcall = detection_model(x.unsqueeze(0))[0][1].item()\n",
    "                if prob_birdcall >= 0.5:\n",
    "                    clip_labels += [labels[i]]\n",
    "                else:\n",
    "                    clip_labels += [torch.zeros(len(species))]\n",
    "            if len(clips) > 0:\n",
    "                processed_labels += [clip_labels]\n",
    "                processed_clips += [[x.unsqueeze(0) for x in clips]]\n",
    "\n",
    "        return processed_clips, processed_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.processed_clips)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        segments = self.processed_clips[index]\n",
    "        segment_labels = self.labels[index]\n",
    "\n",
    "        segment_index = np.random.randint(low = 0, high = len(segments))\n",
    "\n",
    "        x = segments[segment_index]\n",
    "        label = segment_labels[segment_index]\n",
    "\n",
    "        # Add pink noise\n",
    "        if self.use_pink_noise and self.training:\n",
    "            x = add_pink_noise(x, np.random.uniform(low = MIN_SNR, high = MAX_SNR))\n",
    "\n",
    "        # Process\n",
    "        x = spectrogram_transform(x)\n",
    "        if self.use_mel:\n",
    "            x = mel_spectrogram_transform(x)\n",
    "        if self.training:\n",
    "            exponent = np.random.uniform(low = 0.5, high = 3)\n",
    "            x = torch.pow(x, exponent)\n",
    "        x = db_scaler(x)\n",
    "        if self.use_time_mask and self.training:\n",
    "            x = time_mask(x)\n",
    "        if self.use_freq_mask and self.training:\n",
    "            x = freq_mask(x)\n",
    "        x = resize(x)\n",
    "        return x, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# ARCHITECTURE\n",
    "################################################################################\n",
    "\n",
    "\n",
    "class BirdClassifier(nn.Module):\n",
    "    ''' Full architecture from https://github.com/musikalkemist/pytorchforaudio/blob/main/10%20Predictions%20with%20sound%20classifier/cnn.py'''\n",
    "    def __init__(self, num_classes):\n",
    "        super(BirdClassifier, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=16,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=16,\n",
    "                out_channels=32,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=128,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=128,\n",
    "                out_channels=128,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout()\n",
    "        )\n",
    "        self.conv6 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=128,\n",
    "                out_channels=128,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout()\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(46208, num_classes)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        x = self.conv1(input_data)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# WEIGHTED RANDOM SAMPLER SETUP\n",
    "################################################################################\n",
    "\n",
    "#label_counts = data_train['primary_label'].value_counts()\n",
    "\n",
    "#total_durs = data_train.groupby(by='primary_label')['duration'].sum()\n",
    "#total_durs = data_train['primary_label'].apply(lambda x: total_durs[x])\n",
    "\n",
    "#weights = data_train['primary_label'].apply(lambda x: 1/label_counts.loc[x])*(data_train['duration']/total_durs)\n",
    "#weights = data_train['duration']/total_durs\n",
    "#weighted_sampler = torch.utils.data.WeightedRandomSampler(weights.to_list(), len(weights))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# TRAINING SETUP\n",
    "################################################################################\n",
    "\n",
    "# Create a saving directory if needed\n",
    "output_dir = Path(f'{CHECKPOINT_DIR}{MODEL_NAME}')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_dir = f'{CHECKPOINT_DIR}{MODEL_NAME}'\n",
    "\n",
    "# Instantiate our training dataset\n",
    "train_dataset = BirdDataset(signals = data_train['signal'].to_list(), \n",
    "                            labels = data_train['tensor_label'].to_list(),\n",
    "                            training = True,\n",
    "                            use_pink_noise = True)\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=True)\n",
    "\n",
    "# Instantiate our validation dataset\n",
    "validation_dataset =  BirdDataset(signals = data_validation['signal'].to_list(), \n",
    "                                  labels = data_validation['tensor_label'].to_list(),\n",
    "                                  training = False)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Instantiate our model\n",
    "model = BirdClassifier(NUM_SPECIES).to(device)\n",
    "\n",
    "# Set our loss function and optimizer\n",
    "pos_weight = torch.ones([len(species)]).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(pos_weight=pos_weight).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
    "\n",
    "# Training loop\n",
    "print(f\"Training on {len(train_dataset)} samples with {BATCH_SIZE} samples per batch.\")\n",
    "if REPORT_VALIDATION_LOSS_PER_EPOCH == True:\n",
    "    print(f\"Validating on {len(validation_dataset)} samples at the end of each epoch.\")\n",
    "\n",
    "training_losses = [None]*NUM_EPOCHS\n",
    "validation_losses = [None]*NUM_EPOCHS\n",
    "\n",
    "torch.enable_grad() # Turn on the gradient\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# SAVE AND REPORT train test split\n",
    "################################################################################\n",
    "\n",
    "# Save train test split\n",
    "data_train[['primary_label', 'filepath']].to_csv(f\"{output_dir}/data_train.csv\", index = False)\n",
    "data_validation[['primary_label', 'filepath']].to_csv(f\"{output_dir}/data_validation.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TRAINING LOOP\n",
    "################################################################################\n",
    "\n",
    "for epoch_num, epoch in enumerate(tqdm(range(NUM_EPOCHS), leave = False)):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(tqdm(train_dataloader, leave = False)):\n",
    "        \n",
    "        # Get batch of inputs and true labels, push to device\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass on batch of inputs\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Save checkpoint\n",
    "    if SAVE_CHECKPOINTS == True:\n",
    "        torch.save(model.state_dict(), f\"{CHECKPOINT_DIR}{MODEL_NAME}/checkpoint_{epoch_num+1}.pt\")\n",
    "\n",
    "    # Compute training loss\n",
    "    if REPORT_TRAINING_LOSS_PER_EPOCH == True:    \n",
    "        training_losses[epoch_num] = running_loss/len(train_dataloader)\n",
    "        \n",
    "    # Compute validation loss\n",
    "    if REPORT_VALIDATION_LOSS_PER_EPOCH == True:\n",
    "        validation_loss = 0.0\n",
    "        model.eval()\n",
    "        torch.no_grad()\n",
    "        for validation_data in validation_dataloader:\n",
    "            inputs, labels = validation_data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            validation_loss += criterion(outputs, labels).item()\n",
    "        validation_losses[epoch_num] = validation_loss/len(validation_dataloader)\n",
    "        model.train()\n",
    "        torch.enable_grad()\n",
    "\n",
    "    # Save losses\n",
    "    losses = pd.DataFrame({\"training_losses\":training_losses, \"validation_losses\":validation_losses})\n",
    "    cols = []\n",
    "    if REPORT_TRAINING_LOSS_PER_EPOCH == True:\n",
    "        cols += [\"training_losses\"]\n",
    "    if REPORT_VALIDATION_LOSS_PER_EPOCH == True:\n",
    "        cols += [\"validation_losses\"]\n",
    "    if len(cols) > 0:\n",
    "        losses[cols].to_csv(f'{output_dir}/losses.csv', index = False)\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# SAVE AND REPORT \n",
    "################################################################################\n",
    "\n",
    "# Save model\n",
    "if SAVE_AFTER_TRAINING == True:\n",
    "    torch.save(model.state_dict(), f'{output_dir}/final.pt')\n",
    "\n",
    "################################################################################\n",
    "################################################################################\n",
    "################################################################################\n",
    "################################################################################\n",
    "################################################################################\n",
    "################################################################################\n",
    "################################################################################\n",
    "################################################################################\n",
    "################################################################################\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
