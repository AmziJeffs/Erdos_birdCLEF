{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convenience and saving\n",
    "ABRIDGED_RUN = False # Set to True to train and validate on 10% of the data, for quick funcitonality tests etc\n",
    "SAVE_AFTER_TRAINING = True # Save the model when you are done\n",
    "SAVE_CHECKPOINTS = True # Save the model after ever epoch\n",
    "REPORT_TRAINING_LOSS_PER_EPOCH = True # Track the training loss each epoch, and write it to a file after training\n",
    "REPORT_VALIDATION_LOSS_PER_EPOCH = True # Lets us make a nice learning curve after training\n",
    "\n",
    "# Training hyperparameters\n",
    "BATCH_SIZE = 256 # Number of samples per batch while training our network\n",
    "NUM_EPOCHS = 3 # Number of epochs to train our network\n",
    "LEARNING_RATE = 0.001 # Learning rate for our optimizer\n",
    "\n",
    "# Directories\n",
    "DATA_DIR = \"data/\"\n",
    "AUDIO_DIR = DATA_DIR + \"train_audio/\"\n",
    "CHECKPOINT_DIR = \"checkpoints/\" # Checkpoints, models, and training data will be saved here\n",
    "MODEL_NAME = None\n",
    "\n",
    "# Preprocessing info\n",
    "SAMPLE_RATE = 32000 # All our audio uses this sample rate\n",
    "SAMPLE_LENGTH = 5 # Duration we want to crop our audio to\n",
    "NUM_SPECIES = 182 # Number of bird species we need to label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import librosa\n",
    "import os\n",
    "import IPython.display as ipd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch imports\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchaudio.transforms import MelSpectrogram, Resample\n",
    "from IPython.display import Audio\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv(DATA_DIR+\"train_metadata.csv\")\n",
    "data['filepath'] = AUDIO_DIR + data['filename']\n",
    "\n",
    "# We only need the filepath and species label\n",
    "data = data[['filepath', 'primary_label']]\n",
    "\n",
    "# Replace string labels by tensors whose entries are dummies\n",
    "species = data['primary_label'].unique()\n",
    "species_to_index = {species[i]:i for i in range(len(species))}\n",
    "data['tensor_label'] = pd.Series(pd.get_dummies(data['primary_label']).astype(int).values.tolist()).apply(lambda x: torch.Tensor(x))\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split, stratified by species\n",
    "data_train, data_test = train_test_split(data, test_size = 0.2, stratify=data['primary_label'])\n",
    "\n",
    "if ABRIDGED_RUN == True:\n",
    "    data_train = data_train.sample(int(len(data_train)*0.1))\n",
    "    data_test = data_test.sample(int(len(data_train)*0.1))\n",
    "\n",
    "print(data_train.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing and Generating Spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms audio signal to a spectrogram\n",
    "spectrogram_transform = torchaudio.transforms.Spectrogram(\n",
    "        n_fft=2048,\n",
    "        win_length=2048,\n",
    "        hop_length=512,\n",
    "        power=2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts ordinary spectrogram to Mel scale\n",
    "mel_spectrogram_transform = torchaudio.transforms.MelScale(\n",
    "    n_mels=256,\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    f_min=0,\n",
    "    f_max=16000,\n",
    "    n_stft=1025  # the number of frequency bins in the spectrogram\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scales decibels to reasonable level (apply to a spectrogram or Mel spectrogram)\n",
    "db_scaler = torchaudio.transforms.AmplitudeToDB(stype=\"power\", top_db=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resizes spectrograms into square images\n",
    "resize = transforms.Resize((224, 224), antialias = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processes a sample to a tensor for our network\n",
    "def sample_to_tensor(sample):\n",
    "    x = spectrogram_transform(sample)\n",
    "    x = mel_spectrogram_transform(x)\n",
    "    x = db_scaler(x)\n",
    "    x = resize(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a filepath and outputs a torch tensor with shape (1, 224, 224) \n",
    "# that we can feed into our CNN\n",
    "def filepath_to_tensor(filepath):\n",
    "    sample, _ = torchaudio.load(filepath)\n",
    "    if len(sample) >= SAMPLE_RATE * SAMPLE_LENGTH:\n",
    "        sample = sample[:SAMPLE_RATE * SAMPLE_LENGTH]\n",
    "    else:\n",
    "        pad_length = SAMPLE_RATE * SAMPLE_LENGTH - len(sample)\n",
    "        sample = torch.nn.functional.pad(sample, (0, pad_length))\n",
    "    return sample_to_tensor(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a random spectrogram\n",
    "t = filepath_to_tensor(data_train.sample()['filepath'].iloc[0])\n",
    "plt.imshow(t.squeeze().numpy(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Set up torch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: filepaths and labels should be ordinary lists\n",
    "class BirdDataset(Dataset):\n",
    "    def __init__(self, filepaths, labels):\n",
    "        super().__init__()\n",
    "        self.filepaths = filepaths\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filepaths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        processed_clip = filepath_to_tensor(self.filepaths[index])\n",
    "        return processed_clip, self.labels[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Defining neural network architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdClassifier(nn.Module):\n",
    "    ''' Pared down architecture from https://github.com/musikalkemist/pytorchforaudio/blob/main/10%20Predictions%20with%20sound%20classifier/cnn.py'''\n",
    "    def __init__(self, num_classes):\n",
    "        super(BirdClassifier, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=16,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=16,\n",
    "                out_channels=32,\n",
    "                kernel_size=5,\n",
    "                stride=2,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(25088, num_classes)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        x = self.conv1(input_data)\n",
    "        x = self.conv2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device we'll train on\n",
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model name if not set.\n",
    "# Defaults to a timestamp, YYYY-MM-DD_HH_MM_SS\n",
    "if MODEL_NAME == None:\n",
    "    MODEL_NAME = str(pd.Timestamp.now()).replace(\" \", \"_\").replace(\":\", \"-\").split(\".\")[0]\n",
    "\n",
    "# Create a saving directory if needed\n",
    "if SAVE_AFTER_TRAINING or SAVE_CHECKPOINTS or REPORT_TRAINING_LOSS_PER_EPOCH or REPORT_VALIDATION_LOSS_PER_EPOCH:\n",
    "    output_dir = Path(f'{CHECKPOINT_DIR}{MODEL_NAME}')\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate our training dataset\n",
    "train_dataset = BirdDataset(filepaths = data_train['filepath'].to_list(), labels = data_train['tensor_label'].to_list())\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate our validation dataset\n",
    "validation_dataset =  BirdDataset(filepaths = data_test['filepath'].to_list(), labels = data_test['tensor_label'].to_list())\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate our model\n",
    "model = BirdClassifier(NUM_SPECIES).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set our loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "print(f\"Training on {len(train_dataset)} samples with {BATCH_SIZE} samples per batch.\")\n",
    "if REPORT_VALIDATION_LOSS_PER_EPOCH == True:\n",
    "    print(f\"Validating on {len(validation_dataset)} samples at the end of each epoch.\")\n",
    "\n",
    "training_losses = [None]*NUM_EPOCHS\n",
    "validation_losses = [None]*NUM_EPOCHS\n",
    "\n",
    "torch.enable_grad() # Turn on the gradient\n",
    "\n",
    "for epoch_num, epoch in enumerate(tqdm(range(NUM_EPOCHS))):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(tqdm(train_dataloader, leave = False)):\n",
    "        \n",
    "        # Get batch of inputs and true labels, push to device\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass on batch of inputs\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Save checkpoint\n",
    "    if SAVE_CHECKPOINTS == True:\n",
    "        torch.save(model.state_dict(), f\"{CHECKPOINT_DIR}{MODEL_NAME}/checkpoint_{epoch_num+1}.pt\")\n",
    "\n",
    "    # Compute training loss\n",
    "    if REPORT_TRAINING_LOSS_PER_EPOCH == True:    \n",
    "        training_losses[epoch_num] = running_loss/len(train_dataloader)\n",
    "        \n",
    "    # Compute validation loss\n",
    "    if REPORT_VALIDATION_LOSS_PER_EPOCH == True:\n",
    "        validation_loss = 0.0\n",
    "        model.eval()\n",
    "        for validation_data in validation_dataloader:\n",
    "            inputs, labels = validation_data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            validation_loss += criterion(outputs, labels).item()\n",
    "        validation_losses[epoch_num] = validation_loss/len(validation_dataloader)\n",
    "        model.train()\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Save the model and report results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_AFTER_TRAINING == True:\n",
    "    torch.save(model.state_dict(), f\"{CHECKPOINT_DIR}{MODEL_NAME}/final.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = pd.DataFrame({\"training_losses\":training_losses, \"validation_losses\":validation_losses})\n",
    "cols = []\n",
    "if REPORT_TRAINING_LOSS_PER_EPOCH == True:\n",
    "    cols += [\"training_losses\"]\n",
    "if REPORT_VALIDATION_LOSS_PER_EPOCH == True:\n",
    "    cols += [\"validation_losses\"]\n",
    "if len(cols) > 0:\n",
    "    losses[cols].to_csv(f\"{CHECKPOINT_DIR}{MODEL_NAME}/losses.csv\")\n",
    "    print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
