{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import librosa\n",
    "import os\n",
    "import IPython.display as ipd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch imports\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchaudio.transforms import MelSpectrogram, Resample\n",
    "from IPython.display import Audio\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "DATA_DIR = \"data/\"\n",
    "AUDIO_DIR = DATA_DIR + \"train_audio/\"\n",
    "\n",
    "# Preprocessing info\n",
    "SAMPLE_RATE = 32000 # All our audio uses this sample rate\n",
    "SAMPLE_LENGTH = 5 # Duration we want to crop our audio to\n",
    "NUM_SPECIES = 182 # Number of bird species we need to label\n",
    "\n",
    "# Training hyperparameters\n",
    "BATCH_SIZE = 128 # Number of samples per batch while training our network\n",
    "NUM_EPOCHS = 3 # Number of epochs to train our network\n",
    "LEARNING_RATE = 0.001 # Learning rate for our optimizer\n",
    "\n",
    "# Convenience and saving\n",
    "ABRIDGED_TRAINING = False # Set to True to train on 10% of the training data, for quick funcitonality tests etc\n",
    "SAVE_AFTER_TRAINING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv(DATA_DIR+\"train_metadata.csv\")\n",
    "data['filepath'] = AUDIO_DIR + data['filename']\n",
    "\n",
    "# We only need the filepath and species label\n",
    "data = data[['filepath', 'primary_label']]\n",
    "\n",
    "# Replace string labels by tensors whose entries are dummies\n",
    "species = data['primary_label'].unique()\n",
    "species_to_index = {species[i]:i for i in range(len(species))}\n",
    "data['tensor_label'] = pd.Series(pd.get_dummies(data['primary_label']).astype(int).values.tolist()).apply(lambda x: torch.Tensor(x))\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split, stratified by species\n",
    "data_train, data_test = train_test_split(data, test_size = 0.2, stratify=data['primary_label'])\n",
    "\n",
    "if ABRIDGED_TRAINING == True:\n",
    "    data_train = data_train.sample(int(len(data_train)*0.1))\n",
    "\n",
    "print(data_train.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing and Generating Spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms audio signal to a spectrogram\n",
    "spectrogram_transform = torchaudio.transforms.Spectrogram(\n",
    "        n_fft=2048,\n",
    "        win_length=2048,\n",
    "        hop_length=512,\n",
    "        power=2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts ordinary spectrogram to Mel scale\n",
    "mel_spectrogram_transform = torchaudio.transforms.MelScale(\n",
    "    n_mels=256,\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    f_min=0,\n",
    "    f_max=16000,\n",
    "    n_stft=1025  # the number of frequency bins in the spectrogram\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scales decibels to reasonable level (apply to a spectrogram or Mel spectrogram)\n",
    "db_scaler = torchaudio.transforms.AmplitudeToDB(stype=\"power\", top_db=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resizes spectrograms into square images\n",
    "resize = transforms.Resize((224, 224), antialias = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a filepath and outputs a torch tensor with shape (1, 224, 224) \n",
    "# that we can feed into our CNN\n",
    "def filepath_to_tensor(filepath):\n",
    "    sample, _ = torchaudio.load(filepath)\n",
    "    if len(sample) >= SAMPLE_RATE * SAMPLE_LENGTH:\n",
    "        sample = sample[:SAMPLE_RATE * SAMPLE_LENGTH]\n",
    "    else:\n",
    "        pad_length = SAMPLE_RATE * SAMPLE_LENGTH - len(sample)\n",
    "        sample = torch.nn.functional.pad(sample, (0, pad_length))\n",
    "    spec = spectrogram_transform(sample)\n",
    "    mel_spec = mel_spectrogram_transform(spec)\n",
    "    db_scaled_mel_spec = db_scaler(mel_spec)\n",
    "    resized = resize(db_scaled_mel_spec)\n",
    "    return resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a random spectrogram\n",
    "t = filepath_to_tensor(data_train.sample()['filepath'].iloc[0])\n",
    "plt.imshow(t.squeeze().numpy(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Defining Our Torch Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: filepaths and labels should be ordinary lists\n",
    "class birdCLEF_dataset(Dataset):\n",
    "    def __init__(self, filepaths, labels):\n",
    "        super().__init__()\n",
    "        self.filepaths = filepaths\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filepaths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        processed_clip = filepath_to_tensor(self.filepaths[index])\n",
    "        return processed_clip, self.labels[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Defining neural network architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class birdClassifier(nn.Module):\n",
    "    ''' Pared down architecture from https://github.com/musikalkemist/pytorchforaudio/blob/main/10%20Predictions%20with%20sound%20classifier/cnn.py'''\n",
    "    def __init__(self, num_classes):\n",
    "        super(birdClassifier, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=16,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=16,\n",
    "                out_channels=32,\n",
    "                kernel_size=5,\n",
    "                stride=2,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(25088, num_classes)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        x = self.conv1(input_data)\n",
    "        x = self.conv2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device we'll train on\n",
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate our dataset\n",
    "train_dataset = birdCLEF_dataset(filepaths = data_train['filepath'].to_list(), labels = data_train['tensor_label'].to_list())\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate our model\n",
    "model = birdClassifier(NUM_SPECIES).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set our loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "print_per_n_batches = 10\n",
    "print(f\"Training on {len(train_dataset)} samples with {BATCH_SIZE} samples per batch.\")\n",
    "\n",
    "torch.enable_grad() # Turn on the gradient\n",
    "\n",
    "for epoch in tqdm(range(NUM_EPOCHS)):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(tqdm(train_dataloader, leave = False)):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % print_per_n_batches == print_per_n_batches-1:    # print after some batches\n",
    "            print(f'[{epoch + 1}/{NUM_EPOCHS}, {(i + 1):5d}/{len(train_dataloader)}] loss: {running_loss / print_per_n_batches:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_AFTER_TRAINING == True:\n",
    "    torch.save(model.state_dict(), str(pd.Timestamp.now())+\".pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset =  birdCLEF_dataset(filepaths = data_test['filepath'].to_list(), labels = data_test['tensor_label'].to_list())\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "torch.no_grad() # Turn off gradient for fast validation loss computations\n",
    "model.eval()\n",
    "\n",
    "losses = []\n",
    "for i, data in enumerate(tqdm(test_dataloader)):\n",
    "    inputs, labels = data\n",
    "    pred = model(inputs)\n",
    "    losses += [float(criterion(pred, labels))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = pd.Series(losses)\n",
    "losses.index = data_test.index\n",
    "data_test['loss'] = losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average loss on test data: {data_test['loss'].sum() / len(data_test['loss'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_loss_by_species = pd.DataFrame(data_test.groupby(by = ['primary_label'])['loss'].mean()).sort_values(by='loss', ascending = False)\n",
    "fig, ax = plt.subplots(figsize=(24, 8))\n",
    "freqs = sns.barplot(data=average_loss_by_species, x='primary_label', y='loss', ax=ax)\n",
    "freqs.set_xlabel(\"\")\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_sample_per_species = data_train['primary_label'].value_counts().reindex(average_loss_by_species.index)\n",
    "number_sample_per_species = pd.DataFrame(number_sample_per_species)\n",
    "fig, ax = plt.subplots(figsize=(24, 8))\n",
    "freqs = sns.barplot(data=number_sample_per_species, x='primary_label', y='count', ax=ax)\n",
    "freqs.set_title(\"Number of training samples per species\")\n",
    "freqs.set_xlabel(\"primary_label\")\n",
    "freqs.set_ylabel(\"Count\")\n",
    "plt.xticks(rotation=90);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
